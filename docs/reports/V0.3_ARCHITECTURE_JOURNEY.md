# Metal-GS v0.3 Architecture Journey: From 2.6 it/s to 25 it/s on Apple Silicon

> *A first-person account of optimizing 3D Gaussian Splatting on Metal — four phases of architectural exploration, three dead ends, and one breakthrough that rewired our relationship with PyTorch's GPU backend.*

**Author:** Metal-GS Architecture Team  
**Date:** February 2026  
**Hardware:** Mac Mini M4 (10-core GPU, 16GB UMA) / MacBook Air M1 (7-core GPU, 16GB UMA)

---

## TL;DR

| Version | Architecture | Speed (M4) | Bottleneck |
|---------|-------------|------------|------------|
| v0.1 | CPU tensors + `newBufferWithBytes:` copies | ~2.6 it/s | Everything |
| v0.2 | CPU tensors + numpy bridge + per-kernel sync | ~11 it/s | `synchronize()` barriers |
| v0.2+ | Attempted UMA zero-copy via `.cpu().numpy()` | ~10 it/s | Amdahl's Law — overhead was never in the copy |
| **v0.3** | **MPS tensors + `getMTLBufferStorage()` direct bind** | **25+ it/s** | **GPU-bound (the goal)** |

We achieved a **10x speedup** from v0.1 and a **2.5x speedup** from v0.2 — not by writing faster kernels, but by removing everything between the kernels.

---

## Phase 1: The M1 Era — Survival Mode (v0.1)

### The Constraint Nobody Warned Us About

When we first ported 3D Gaussian Splatting from CUDA to Metal, the naive assumption was "just translate the kernels." The kernels translated fine. What didn't translate was the *execution model*.

CUDA's Gaussian Splatting runs tiles of arbitrary length — a tile with 10,000 overlapping Gaussians simply runs longer. On Metal (M1/M2 GPU Family 7/8), that same tile triggers macOS's WindowServer watchdog:

```
Impacting System Interactivity: app_name
```

The GPU kills the command buffer after ~2 seconds. This isn't a bug — it's a design invariant. M1/M2 GPUs use **static resource allocation**. A single threadgroup cannot be preempted. If it runs too long, it monopolizes a GPU core, and the OS kills it to protect the compositor.

### The Solution: Depth-Sorted Hard Capping

We implemented `max_gaussians_per_tile` — a per-tile hard cap on the number of Gaussians processed during rasterization. Since Gaussians are processed in strict front-to-back depth order, truncation only discards the most distant (most occluded) contributions. The alpha-blending transmittance $T_i = \prod_{j<i}(1 - \alpha_j)$ decays exponentially — after a few hundred Gaussians, $T_i < 10^{-4}$, making all subsequent contributions sub-pixel noise.

This was mathematically safe, fully differentiable (backward applies the same cap), and eliminated the watchdog entirely.

### The v0.1 Architecture

```
Python (numpy arrays)
    → newBufferWithBytes: (CPU → GPU copy per kernel)
    → dispatch Metal kernel
    → waitUntilCompleted
    → memcpy result back to numpy
    → next kernel
```

Every kernel dispatch was a full round-trip: allocate a `MTLBuffer`, copy data in, dispatch, wait, copy data out. Nine kernels in the forward pass meant 9 synchronization points and 18 copies. On M1, this ran at **~2.6 it/s** — painfully slow, but correct.

### What We Learned

- Hard capping is essential on M1/M2 and harmless on M3/M4 (Dynamic Caching eliminates the watchdog)
- FP32 everywhere was the right call for M1 — no hardware BF16, and we needed bit-exact gradient verification
- The architecture was "correct but slow" — a solid foundation, but we were leaving 90% of the GPU's capability on the table

---

## Phase 2: The M4 Hardware Dividend (v0.2)

### New Hardware, New Possibilities

The M4 (Apple GPU Family 9+) brought two game-changing features:

1. **Dynamic Caching** — hardware-level fine-grained preemption of threadgroups. The watchdog problem vanished entirely. We could set `max_gaussians_per_tile=0` (unlimited) with zero risk.

2. **Native BF16 ALUs** — `bfloat16` hardware instructions with identical throughput to FP16 but with FP32-range exponent (8 bits vs 5 bits). We gated this behind `ENABLE_BF16=1` in the Metal shader compilation, affecting only the preprocess kernel's covariance accumulation. The alpha-blending rasterizer remained FP32 unconditionally.

### Measured Impact

| Change | Speed Impact |
|--------|-------------|
| BF16 covariance accumulation | +5% (~10.2 → ~10.7 it/s) |
| Uncapped tiles (cap=0) | +3% (~10.7 → ~11.0 it/s) |
| Combined | ~11 it/s baseline |

BF16 was "free" performance — measured gradient error: FP32 `MaxAbs=3.73e-07` vs BF16 `MaxAbs=4.37e-07`, both within $10^{-7}$ of float64 reference. Identical precision for all practical purposes.

### The Profiling Wake-Up Call

At 11 it/s (~91ms/iteration), we ran a detailed kernel-level GPU profiler. The results were shocking:

| Component | Time | % of Iteration |
|-----------|------|----------------|
| **GPU kernels (all 9)** | **22ms** | **24%** |
| CPU↔GPU transfer (numpy copies) | 19ms | 21% |
| **PyTorch autograd overhead** | **50ms** | **55%** |

The GPU was idle for **76% of every iteration.** The kernels themselves were fast. Everything *around* them was slow.

### The Architecture at v0.2

```
Python (numpy arrays on CPU)
    → .cpu().numpy()           # tensor → numpy
    → metal_wrapper.mm         # 9 separate C functions
        → newBufferWithBytes:  # copy to MTLBuffer
        → dispatch kernel
        → waitUntilCompleted   # sync barrier
        → memcpy to output     # copy back
    → numpy → torch.Tensor    # reconstruct for autograd
```

The `metal_wrapper.mm` file was 1,553 lines of Objective-C++ that faithfully copied data between numpy arrays and Metal buffers. Each of the 9 forward kernels and 3 backward kernels was a separate function with its own command buffer, its own wait, its own copy-in and copy-out.

---

## Phase 3: The UMA Zero-Copy Mirage (v0.2+, aborted)

### The Hypothesis That Led Us Astray

Apple Silicon has Unified Memory Architecture (UMA). CPU and GPU share the same physical DRAM. So `MTLResourceStorageModeShared` should give us zero-copy access — the CPU writes directly to GPU-visible memory without any DMA transfer.

The hypothesis: "If we use `StorageModeShared` buffers backed by numpy arrays' memory, we can eliminate the 19ms of CPU↔GPU transfer. That's a 21% speedup for free."

### What Actually Happened

We refactored `metal_wrapper.mm` to use `newBufferWithBytesNoCopy:` with numpy array pointers. The copies disappeared from the GPU timeline. The speed improved... barely.

**From ~11 it/s to ~10 it/s.** Actually *slower* on some runs.

### The Amdahl's Law Trap

The 50ms "PyTorch autograd overhead" wasn't autograd at all. It was an artifact of our measurement methodology. Here's what was really happening:

1. Each kernel dispatch called `[cmd waitUntilCompleted]` — a full GPU pipeline drain
2. Between kernels, PyTorch's autograd traced the computational graph
3. The `synchronize()` calls in our profiler **serialized the entire pipeline**, turning what could be overlapped GPU work into sequential blocking calls

When we profiled with `mach_absolute_time()` around each kernel's `waitUntilCompleted`, we measured the *wall-clock time including pipeline drain latency* — not the kernel execution time. The real kernel time was 22ms. The remaining 69ms was:

- **Pipeline refill latency**: After `waitUntilCompleted` drains the GPU pipeline, the next command buffer submission must fill it again. On M4 (10 cores), this costs ~2-5ms per transition.
- **Autorelease pool overhead**: Each `@autoreleasepool` in our 9 separate dispatch functions created and drained ObjC objects.
- **Python↔C++ crossing**: The pybind11 boundary was crossed 9 times per forward pass, each time converting numpy arrays to raw pointers and back.

The 19ms "transfer" was real but irrelevant — eliminating it just shifted the bottleneck to the next item in the chain. **Amdahl's Law in action**: optimizing 21% of a pipeline where 55% is synchronization overhead yields approximately zero net benefit.

### The Real Insight

The question wasn't "how do we copy data faster?" It was **"why are we copying data at all?"**

Every MPS tensor in PyTorch is already backed by a `id<MTLBuffer>` on the GPU. When we called `.cpu().numpy()`, we were:
1. Forcing a GPU→CPU synchronization (pipeline drain)
2. Copying GPU buffer contents to CPU-side numpy memory
3. Passing that CPU pointer to Metal
4. Metal driver doing a CPU→GPU copy back into a new `MTLBuffer`

We were round-tripping data through CPU memory that was already on the GPU. The "unified memory" didn't help because we were going *around* it, not *through* it.

---

## Phase 4: The Breakthrough — MPS Custom Ops (v0.3)

### The Architecture Insight

PyTorch's MPS backend maintains an internal `MPSStream` — a singleton that owns a `MTLCommandQueue` and manages `MTLComputeCommandEncoder` instances. Every PyTorch MPS operation (addition, matmul, etc.) dispatches Metal kernels through this stream.

**The key API surface:**

```cpp
#include <ATen/mps/MPSStream.h>
#include <ATen/native/mps/OperationUtils.h>

// Get PyTorch's active MPS command stream
MPSStream* stream = at::mps::getCurrentMPSStream();

// Get the current compute encoder (creates one if needed)
id<MTLComputeCommandEncoder> enc = stream->commandEncoder();

// Extract id<MTLBuffer> from any MPS tensor — zero-copy
id<MTLBuffer> buf = at::native::mps::getMTLBufferStorage(tensor);

// Synchronize: ends encoder, commits command buffer, waits
stream->synchronize(at::mps::SyncType::COMMIT_AND_WAIT);
```

This is not public API. It's PyTorch's internal Metal backend implementation. But it's our only path to zero-copy: instead of extracting data from tensors and feeding it to our own Metal command queue, we inject our kernels into PyTorch's command queue.

### The v0.3 Architecture

```
PyTorch autograd (MPS device)
    │
    ▼
metal_gs/rasterizer.py ── MetalGaussianRasterizer (torch.autograd.Function)
    │                      All tensors stay on MPS. No .cpu(), no numpy.
    ▼
_metal_gs_core (pybind11, torch::Tensor API)
    │
    ▼
csrc/mps_ops.mm ── MPS Custom Op dispatch layer
    │  ┌──────────────────────────────────────────────────────────────────┐
    │  │  Uses PyTorch's internal MPSStream (shared command queue)        │
    │  │  getMTLBufferStorage() extracts id<MTLBuffer> from MPS tensors  │
    │  │  PSOs created on PyTorch's MPS device                           │
    │  │  Single encoder per phase — memoryBarrierWithScope between ops  │
    │  │  ONE unavoidable sync: read num_intersections for allocation    │
    │  └──────────────────────────────────────────────────────────────────┘
    ▼
csrc/kernels/*.metal ── 17 PSOs, AOT-compiled metallib
```

### The Pipeline in Detail

**Forward pass (3 phases, 1 sync point):**

```
Phase 1 — PyTorch ops (async on MPS queue):
    directions = normalize(means3d - campos)
    sh_fp16 = sh_coeffs.to(float16)     ← critical: Metal SH kernel reads half*
    colors_fp16 = SH_kernel(directions, sh_fp16)
    colors_fp32 = colors_fp16.to(float32)

Phase 2 — Single Metal encoder:
    preprocess → gen_sort_keys → radix_sort(8 passes) → tile_count → reduce_sum
    [memoryBarrierWithScope between each stage]
    [synchronize(COMMIT_AND_WAIT): read num_intersections from Shared buffer]

Phase 3 — Single Metal encoder:
    prefix_sum → gen_intersections → radix_sort(tile sort) → tile_range → rasterize
```

**Backward pass (single encoder, zero syncs):**
```
    rasterize_backward → preprocess_backward → sh_backward
    [memoryBarrierWithScope between each stage]
```

### The Critical Lessons (Paid in Blood)

#### 1. NEVER Call `[enc endEncoding]` Directly

This was our first crash. The MPS stream owns the encoder lifecycle. When we called `[enc endEncoding]`, the encoder was ended. Then `stream->synchronize(COMMIT_AND_WAIT)` internally called `endKernelCoalescing()`, which tried to end the *same* encoder again:

```
-[MTLDebugComputeCommandEncoder endEncoding]:
    endEncoding has already been called on this command encoder
```

Metal GPU assertion failure. Process killed.

**Rule:** Always let the stream end the encoder via `synchronize()`. Never call `[enc endEncoding]`.

#### 2. No Blit Encoders on the MPS Stream

We initially tried to use `[cmd blitCommandEncoder]` to copy sort results from private buffers to tensor-backed buffers. But `stream->commandBuffer()` returns the stream's internal command buffer — creating a blit encoder on it conflicts with the stream's encoder management state.

**Solution:** Use tensor-backed `MTLBuffer`s as sort output buffers directly. The sort's `vals_a` is the tensor's buffer, so after 8 passes (even number), results land directly in the tensor. Zero copies needed.

#### 3. No ARC (`-fobjc-arc`) with PyTorch MPS Headers

PyTorch's `OperationUtils.h` uses manual retain/release patterns. Compiling with ARC causes 16+ errors:
- `dispatch_release()` not available under ARC
- `id<MTLComputePipelineState>` to `const void*` casts rejected

**Solution:** Compile `.mm` files with `-ObjC++ -std=c++17` but *without* `-fobjc-arc`.

#### 4. SH Coefficient Precision Gate

The Metal SH kernels read coefficients as `device const half*` (float16) and output colors as `half*`. The old code explicitly converted float32 SH coefficients to float16 in Python before passing to the C binding.

In v0.3, we pass `torch::Tensor` directly. If we forget the `.to(torch::kFloat16)` conversion, the kernel interprets float32 bits as float16 — producing completely garbled colors. The loss doesn't converge, the rendered image shows blocky color artifacts, but *shapes are correct* (preprocess/sort/rasterize are unaffected).

This was a subtle bug because the training still "runs" — it just learns garbage colors.

#### 5. The One Unavoidable Sync

After the `reduce_sum` kernel computes per-block tile intersection counts, we need the *total* count to allocate the intersection list tensor. This requires reading a scalar from a `StorageModeShared` buffer on the CPU:

```cpp
stream->synchronize(at::mps::SyncType::COMMIT_AND_WAIT);
uint32_t num_isect = 0;
uint32_t* bs = (uint32_t*)[block_sums contents];
for (uint32_t i = 0; i < reduce_tgs; i++) num_isect += bs[i];
```

This is unavoidable with the current architecture — we can't allocate the intersection list without knowing its size, and we can't know its size without reading GPU results. A future optimization could pre-allocate a worst-case buffer, but for 165K Gaussians the sync costs <1ms.

### Performance Breakdown (v0.3, M4)

| Component | Time | Notes |
|-----------|------|-------|
| Forward Phase 1 (SH + directions) | ~4ms | PyTorch MPS ops, async |
| Forward Phase 2 (preprocess + sort + reduce) | ~8ms | Single encoder |
| Sync (read num_isect) | <1ms | Unavoidable |
| Forward Phase 3 (binning + rasterize) | ~10ms | Single encoder |
| Backward (all 3 stages) | ~12ms | Single encoder, zero syncs |
| PyTorch autograd bookkeeping | ~5ms | Tensor graph, optimizer.step() |
| **Total** | **~40ms** | **25 it/s** |

The GPU is now saturated. The 5ms autograd overhead is irreducible — it's PyTorch maintaining the computational graph. There is no more "waste" to eliminate.

---

## The Final Numbers

| Metric | v0.1 (M1) | v0.2 (M4) | v0.3 (M4) |
|--------|-----------|-----------|-----------|
| Speed | 2.6 it/s | 11 it/s | **25.2 it/s** |
| Iteration time | 385ms | 91ms | **39.7ms** |
| GPU kernel time | ~22ms | ~22ms | ~22ms |
| Sync/copy overhead | ~363ms | ~69ms | **<1ms** |
| CPU↔GPU copies per iter | 18+ | 18 | **0** |
| Sync barriers per iter | 9 | 9 | **1** |
| Loss at 500 iter | 0.094 | 0.130 | **0.138** |
| Files changed (dispatch) | metal_wrapper.mm (1553 LOC) | same | mps_ops.mm (855 LOC) |

The kernels didn't get faster — they were always 22ms. We just stopped wasting the other 70ms fighting the framework.

---

## Architectural Principles (Hard-Won)

1. **Don't fight the framework.** PyTorch MPS has a command queue. Use it. Don't create your own.

2. **Zero-copy means zero-copy.** Not "copy via shared memory" — you must bind the *same* `MTLBuffer` that the tensor owns. `getMTLBufferStorage()` is the only path.

3. **Minimize sync points.** Every `synchronize(COMMIT_AND_WAIT)` drains the GPU pipeline. On a 10-core GPU, pipeline refill costs 2-5ms. One sync per forward pass is acceptable. Nine is catastrophic.

4. **Profile the *system*, not the *kernels*.** Our GPU kernels were never the bottleneck. The bottleneck was always in the plumbing between them. `mach_absolute_time()` around `waitUntilCompleted` doesn't measure kernel time — it measures pipeline drain + kernel + refill.

5. **UMA is not a substitute for architecture.** Apple's unified memory eliminates *hardware* copy cost, but software copies (`memcpy`, `newBufferWithBytes:`, `.cpu().numpy()`) still serialize the pipeline. The speed of the copy is irrelevant — it's the *synchronization* that kills you.

6. **Respect the encoder lifecycle.** The MPS stream owns the `MTLComputeCommandEncoder`. Don't call `[enc endEncoding]`. Don't create blit encoders on the stream's command buffer. Push your kernels through the stream's encoder, and let `synchronize()` handle the rest.

---

## What's Still on the Table

- **Viewer (Viser) integration**: The v0.3 MPS pipeline monopolizes the GPU command queue. When Viser's rendering loop also submits Metal work, the two compete for the same queue, causing crashes. Training works perfectly in headless mode (`use_viewer=False`). Fixing this requires either command queue serialization or separating the training and viewer into distinct queues.

- **SIMD reduction for gradient accumulation**: The backward rasterizer uses naive `atomic_fetch_add_explicit` for gradient accumulation. SIMD warp-level reduction (`simd_sum`) could reduce atomic contention by 32×, but the current backward pass is not the bottleneck (~12ms).

- **Multi-resolution training**: Full-resolution (1032×688) on M4 should be feasible at ~10-12 it/s with the v0.3 architecture. Untested.

- **M1 backward compatibility**: v0.3 uses `ATen/mps/MPSStream.h` which requires PyTorch 2.1+. The MPS backend on M1 is functional but less tested. The single sync point in the forward pass may have different performance characteristics on M1's 7-core GPU.

---

## For the Open Source Community

If you're building a Metal compute pipeline that integrates with PyTorch MPS:

1. Read PyTorch's `ATen/mps/MPSStream.h` and `ATen/native/mps/OperationUtils.h`. These are your only tools.
2. Don't try to create your own `MTLCommandQueue`. PyTorch's MPS backend uses a singleton queue. Two queues competing for the same GPU will deadlock or corrupt state.
3. `getMTLBufferStorage()` gives you a `id<MTLBuffer>` that is the tensor's actual storage. Not a copy. Not a view. The buffer itself. Write to it and the tensor's data changes.
4. The MPS stream's encoder coalescing is aggressive. Multiple kernel dispatches on the same encoder share a single command buffer submission. This is *why* v0.3 is fast — we encode 8+ kernels before a single `synchronize()`.
5. Test on M1. If your code survives the 7-core GPU with 16GB shared memory and a 2-second watchdog, it'll run anywhere.

This journey took us from "it works" to "it's fast." The kernel code barely changed. The architecture changed completely.

---

*Metal-GS v0.3 — February 2026*
