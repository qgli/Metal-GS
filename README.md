# Metal-GS

**3D Gaussian Splatting on Apple Silicon â€” pure Metal compute shaders, no CUDA required.**

Metal-GS is a fully differentiable 3D Gaussian Splatting renderer that runs entirely on Apple Silicon GPUs via Metal compute shaders. It implements the complete forward + backward pipeline (SH evaluation, projection, radix sort, tile binning, alpha-blending rasterization) with all kernels AOT-compiled into a single `.metallib`.

> **Status:** research prototype â€” trains 165K Gaussians at ~2.6 it/s on M1 16GB (516Ã—344).

---

## The Problem: CUDA â†’ Metal Is Not a Transliteration

Porting Gaussian Splatting from CUDA to Metal exposes a fundamental architectural difference: **Metal compute shaders dispatched via `threadgroup_barrier` are non-preemptible on Apple GPU.** On CUDA, a tile with 10,000 overlapping Gaussians simply runs longer. On Metal (M1, 7 GPU cores), the same tile triggers macOS's WindowServer watchdog ("Impacting System Interactivity"), which **kills the command buffer** after ~2 seconds.

This is not a bug â€” it is a design constraint of Apple's GPU architecture where the OS must maintain UI responsiveness.

## The Solution: Depth-Sorted Hard Capping

Since Gaussians are processed in strict front-to-back depth order, we apply a per-tile hard cap (`max_gaussians_per_tile`, default 1024). Dense tiles are truncated, discarding only the most distant (and most occluded) Gaussians. This:

- **Bounds worst-case GPU time** per tile to a fixed constant, eliminating watchdog timeouts
- **Preserves visual quality** â€” truncated Gaussians are behind hundreds of closer ones
- **Is fully differentiable** â€” backward pass applies the same cap, matching forward exactly

The cap is a runtime parameter (not a compile-time constant), tunable per scene via `RenderSettings.max_gaussians_per_tile`. Set to `0` for unlimited (at your own risk on M1).

---

## Architecture

```
PyTorch autograd (CPU/MPS)
    â”‚
    â–¼
metal_gs/rasterizer.py â”€â”€ MetalGaussianRasterizer (autograd.Function)
    â”‚
    â–¼
_metal_gs_core (PyBind11)
    â”‚
    â–¼
csrc/metal_wrapper.mm â”€â”€ ObjC++ dispatch layer
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”œâ”€â”€â”‚  Single MTLCommandQueue, one command buffer per dispatch     â”‚
    â”‚  â”‚  MTLResourceStorageModeShared â€” zero-copy unified memory     â”‚
    â”‚  â”‚  @autoreleasepool on all 9 entry points                      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â–¼
csrc/kernels/*.metal â”€â”€ 15 PSOs, AOT-compiled metallib
    â”œâ”€â”€ sh_forward.metal        SH basis evaluation
    â”œâ”€â”€ preprocess.metal        3Dâ†’2D projection, cov2d, tile bounds
    â”œâ”€â”€ radix_sort.metal        32-bit radix sort (histogram, scan, scatter)
    â”œâ”€â”€ tile_binning.metal      Gaussianâ†’tile assignment + tile_range
    â”œâ”€â”€ rasterize.metal         Forward alpha blending (cooperative fetch)
    â”œâ”€â”€ rasterize_backward.metal  Backward (reverse traversal, atomic grads)
    â”œâ”€â”€ preprocess_backward.metal Projection backward
    â”œâ”€â”€ sh_backward.metal       SH backward
    â””â”€â”€ knn.metal               Morton-code KNN for scale initialization
```

**Key design decisions:**
- **Single encoder per dispatch** â€” no multi-pass command buffer splitting; `memoryBarrierWithScope` between stages
- **3-level prefix sum** for radix sort â€” block-level scan â†’ block-sum scan â†’ scatter; all in one metallib
- **FP32 everywhere** on M1 (`ENABLE_BF16=0`); set to `1` for M4+ with BF16 support
- **Naive atomic gradient accumulation** (Strategy A) â€” correctness-first; SIMD reduction is a future optimization

---

## Quick Start

### Prerequisites

- macOS 13+ with Apple Silicon (M1/M2/M3/M4)
- Xcode Command Line Tools (`xcode-select --install`)
- Python 3.10+, conda or venv

### Install

```bash
# Create environment
conda create -n metal-gs python=3.10 -y
conda activate metal-gs
pip install torch numpy pybind11 tqdm Pillow viser

# Build (AOT-compiles Metal shaders + C++/ObjC++ extension)
cd Metal-GS
CC=/usr/bin/clang CXX=/usr/bin/clang++ pip install -e .
```

### Train on COLMAP Data

Metal-GS includes **minGS**, a minimal training harness:

```bash
cd minGS
python example.py
```

This trains 500 iterations on the bundled COLMAP dataset (165K Gaussians, 179 cameras, 516Ã—344 @ 2x downsample) with a live Viser viewer at [http://localhost:8080](http://localhost:8080), then saves `cat_mac_render.png`.

### Use as a Library

```python
import numpy as np
import torch
from metal_gs.rasterizer import MetalGaussianRasterizer, RenderSettings

settings = RenderSettings(
    viewmat=viewmat_np,            # [4,4] float32
    tan_fovx=tan_fovx,
    tan_fovy=tan_fovy,
    focal_x=focal_x,
    focal_y=focal_y,
    principal_x=W / 2.0,
    principal_y=H / 2.0,
    img_width=W,
    img_height=H,
    sh_degree=3,
    bg_color=(0.0, 0.0, 0.0),
    max_gaussians_per_tile=1024,   # tune for your GPU
)

# Fully differentiable â€” gradients flow through Metal compute shaders
image = MetalGaussianRasterizer.apply(
    means3d, scales, quats, sh_coeffs, opacities,
    viewmat_tensor, campos_tensor, settings
)
loss = (image - target).pow(2).mean()
loss.backward()
```

---

## Configuration

| Parameter | Default | Description |
|---|---|---|
| `max_gaussians_per_tile` | 1024 | Hard cap per tile. Prevents watchdog timeout on M1. Increase for higher quality on M3/M4 Pro. Set `0` for unlimited. |
| `DOWNSAMPLE` | 2 | Image downsampling factor in `example.py`. Use 1 for full-res on M3+ with â‰¥32GB. |
| `ENABLE_BF16` | 0 | Set to 1 in `setup.py` for BF16 training on M4+ (Apple GPU Family 9+). |

---

## Performance (M1 16GB, 7 GPU cores)

| Dataset | Points | Resolution | Cap | Speed | Final Loss |
|---|---|---|---|---|---|
| Cat (COLMAP) | 165K | 516Ã—344 (2x) | 1024 | ~2.6 it/s | 0.094 |
| Cat (COLMAP) | 165K | 1032Ã—688 (1x) | 1024 | ~0.6 it/s | â€” |

---

## Hardware Tested

> **Metal-GS v1.0 has been developed and stress-tested exclusively on the weakest Apple Silicon chip: M1 with 7 GPU cores and 16GB unified memory (~4â€“6GB usable after system overhead).**

This is a deliberate design choice. If the code survives the M1's constraints â€” 7 GPU cores, no hardware ray tracing, no BF16, and macOS's aggressive 2-second GPU watchdog â€” it will run on anything Apple ships.

| Chip | GPU Cores | Memory | Status |
|---|---|---|---|
| **M1 (7-core)** | 7 | 16GB | âœ… **Fully tested** â€” 2000 iterations at full resolution |
| M1 Pro/Max/Ultra | 16â€“64 | 32â€“192GB | ðŸ”œ Expected to work (same ISA, more headroom) |
| M2/M3/M4 family | 8â€“40 | 8â€“192GB | ðŸ”œ Performance testing planned on M4 Max |

**For M3/M4 users:** You can safely increase `max_gaussians_per_tile` beyond 1024 (try 2048â€“4096) and set `DOWNSAMPLE=1` for full-resolution training. M4+ users can also enable `ENABLE_BF16=1` in `setup.py` for mixed-precision training.

---

## Known Limitations

- **Viser real-time visualization:** Verified working on M1 7-core GPU. Training + live Viser rendering runs concurrently without GPU watchdog warnings (~2.7 it/s with viewer active, ~5.5 it/s without). Frame drops may occur under extreme load.
- **No multi-GPU:** Metal-GS targets single-GPU Apple Silicon. Multi-GPU (Mac Pro with multiple M2 Ultra) is not supported.
- **FP32 only on M1:** BF16 requires Apple GPU Family 9+ (M4). M1/M2/M3 run all kernels in FP32.
- **Densification scale:** The bundled minGS trainer uses a simplified densification schedule (30 iterations). For production quality, increase `densify_until_iter`.

---

## Dataset

Metal-GS ships with a COLMAP-processed cat dataset (179 frames, 165K points) for quick testing.

**Download:** The dataset will be available on [Google Drive](https://drive.google.com/) (link coming soon).

### Expected COLMAP Directory Structure

```
data/cat/
â”œâ”€â”€ images/                   # 179 source images (frame_00001.JPG â€¦ frame_00179.JPG)
â”‚   â”œâ”€â”€ frame_00001.JPG
â”‚   â”œâ”€â”€ frame_00002.JPG
â”‚   â””â”€â”€ ...
â””â”€â”€ sparse/
    â””â”€â”€ 0/
        â”œâ”€â”€ cameras.bin       # Camera intrinsics (COLMAP format)
        â”œâ”€â”€ images.bin        # Camera extrinsics (COLMAP format)
        â”œâ”€â”€ points3D.bin      # 3D point cloud (COLMAP format)
        â””â”€â”€ points3D.ply      # Point cloud in PLY format (optional)
```

To use your own data, run COLMAP on your images and place the output in the same structure. Metal-GS reads the standard COLMAP binary format via `minGS/gs/io/colmap/`.

---

## Project Structure

```
Metal-GS/
â”œâ”€â”€ csrc/
â”‚   â”œâ”€â”€ kernels/          9 Metal shader files â†’ AOT-compiled metallib
â”‚   â”œâ”€â”€ metal_wrapper.mm  ObjC++ dispatch layer (9 public functions)
â”‚   â”œâ”€â”€ metal_wrapper.h   C++ header
â”‚   â””â”€â”€ bindings.cpp      PyBind11 bindings
â”œâ”€â”€ metal_gs/
â”‚   â”œâ”€â”€ rasterizer.py     PyTorch autograd wrapper + RenderSettings
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ minGS/                Minimal training harness (COLMAP loader, 3DGS model)
â”‚   â”œâ”€â”€ example.py        Entry point â€” train + render
â”‚   â”œâ”€â”€ gs/               GaussianModel, trainers, visualization
â”‚   â””â”€â”€ data/cat/         Bundled COLMAP dataset
â”œâ”€â”€ setup.py              Build script (Metal AOT + C++/ObjC++ compilation)
â””â”€â”€ pyproject.toml        PEP 517 build-system declaration
```

---

## Acknowledgements

Metal-GS builds on the work of several excellent open-source projects:

- **[gsplat](https://github.com/nerfstudio-project/gsplat)** â€” reference CUDA kernels for the rasterization pipeline. Metal-GS reimplements the gsplat forward/backward kernels as Metal compute shaders with architecture-specific adaptations (cooperative fetch, SIMD prefix sums, hard capping for TBDR).
- **[3D Gaussian Splatting](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)** â€” Kerbl et al. 2023, the foundational algorithm.
- **[minGS](https://github.com/shiukaheng/minGS)** (Shiu Ka Heng) â€” minimal 3DGS training framework. Metal-GS uses a modified fork: replaced CUDA KNN with a Metal Morton-code KNN kernel, removed the CUDA/`diff-gaussian-rasterization` dependency, and wired up MPS tensor support.
- **[mlx-splat](https://github.com/daikiad/mlx-splat)** (daikiad) â€” MLX-native Gaussian Splatting. Studied for Apple Silicon tiling strategy and alpha-blending formulation; Metal-GS takes a different approach (PyTorch MPS + raw Metal compute shaders rather than MLX primitives).
- **[OpenSplat](https://github.com/pierotofy/OpenSplat)** (Piero Toffanin) â€” portable C++ 3DGS with Metal backend via gsplat-metal. Studied for Metal shader dispatch patterns; Metal-GS differs in using a single-encoder architecture with `memoryBarrierWithScope` rather than multi-encoder passes.

---

## License

MIT
